{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "urDyLLZ6MVFI"
   },
   "source": [
    "# Analyse de sentiments de commentaires de films par Machine Learning :\n",
    "***\n",
    "\n",
    "- ## Objectif de l'étude : \n",
    "Faire le tri entre avis positifs et avis négatifs des commentaires de la base de données [IMDB](https://www.imdb.com/) (Internet Movie DataBase) qui contient, parmi bien d'autre choses, des avis sur un grand nombre de films. Puis être capable de prédire automatiquement si un avis est positif ou négatif.\n",
    "\n",
    "- ## Méthodologie :\n",
    "Nous allons donc effectuer une classification binaire (1, positif ou 0, négatif) par Natural Language Processing (NLP). Nous choisissons pour cette tâche d'utiliser un réseau de neurones très simple, construit avec le module Keras (API la plus populaire pour le Deep Learning, basée sur TensorFlow). Par chance (ou pas !), le dataset IMDB est disponible dans Keras, pratique ! Il contient les avis de 25000 films. Il s'agit donc ici de Machine Learning supervisé, car le réseau de neurones va apprendre à partir des avis (associés aux commentaires) déjà existants dans le dataset, pour ensuite prédire par lui-même !\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par importer les modules Python nécessaires : *NumPy* (module de calcul scientifique), et quelques modules de *Keras* (module de Deep Learning inclus dans *TensorFlow*) tels que ``imdb``, et c'est tout ! <br>\n",
    "__NB__ : il faudra donc avoir installé les modules *NumPy* et *TensorFlow* au préalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1yJNO9bIjeF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxkjJ7EQTNOK"
   },
   "source": [
    "## Le dataset IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjXXlDoEW4WB"
   },
   "source": [
    "### Chargement des données\n",
    "On télécharge les 1000 premiers mots du dataset, et on découpe ce dataset en deux : un dataset d'entraînement, et un dataset de test, qui va nous permettre d'évaluer la précision de notre réseau de neurones. <br> ``X`` étant nos commentaires, et ``y`` les avis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-5biWCSW3cw"
   },
   "outputs": [],
   "source": [
    "voc_size = 1000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZIczjwJN8nr"
   },
   "source": [
    "Que contient concrètement ``X`` ?\n",
    "Regardons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "colab_type": "code",
    "id": "PBGFDvo3NkWA",
    "outputId": "e907a7a8-d9c8-4c6f-cd3d-353accfa8d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "       list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 715, 8, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 775, 7, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 625, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 534, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 645, 662, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "       ...,\n",
       "       list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 912, 84, 2, 325, 725, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 563, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 6, 2, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 929, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 747, 2, 372, 2, 2, 541, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
       "       list([1, 2, 2, 69, 72, 2, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 719, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 631, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 544, 5, 383, 2, 848, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDUs2cgUUX1L"
   },
   "source": [
    "On peut être surprit au premier regard, il n'y a pas de texte !\n",
    "\n",
    "\n",
    "**Explication :**\n",
    "Keras nous a prémâché le travail en encodant chaque mot du data set selon sa fréquence d'utilisation (processus appelé *Tokenization*).\n",
    "\n",
    "Par exemple, \"2\", \"33\" qui sont les 1er mots extraits du data set signifient :\n",
    "2 et 33ème mot les plus fréquents. Et ainsi de suite.\n",
    "\n",
    "Maintenant que contient ``y`` (les avis) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f5vvjMvMNopq",
    "outputId": "6ef87cf2-7854-4288-bb6c-a27727102ea8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ki11O1SZJuY"
   },
   "source": [
    "Ce sont les avis : <br>\n",
    "1 : positif\n",
    "0 : négatif\n",
    "\n",
    "C'est ce qu'on veut faire apprendre à notre réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDbY65Y9TWOt"
   },
   "source": [
    "## Préparer le data set\n",
    "Une opération classique en Text Mining avec Keras est de *pader* les séquences (morceaux de phrases considérés).\n",
    "\n",
    "Les séquences trop courtes sont complétées avec des zéros (par défaut). Celles qui sont trop longues sont tronquées.\n",
    "C'est un moyen d'obtenir un tableau 2D de longueur fixe (1 avis, 1 ligne).\n",
    "\n",
    "Les paramètres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDC2IpwyNHqC"
   },
   "outputs": [],
   "source": [
    "voc_size = 1000\n",
    "input_length = 100\n",
    "output_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uqx1EHziNO9a"
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen = input_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = input_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkvWkNztTcGT"
   },
   "source": [
    "## Créer un modèle de Deep Learning avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xu-NF2l9Z0p8"
   },
   "source": [
    "La couche **embedding** permet de transformer chaque mot en vecteur (colonne de chiffres) en tenant compte de son contexte dans la séquence, elle a pour paramètres :\n",
    "* un vocabulaire de longueur ``voc_size`` (nombre de mots pris en compte)\n",
    "* des séquences de longueur  ``input_length`` (longueur des morceaux de phrase considérés)\n",
    "* un vecteur de dimension ``output_dim`` qui contient le mot et son contexte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "6zwQeLi6NTZ6",
    "outputId": "7a303e30-ef3d-41c8-af31-390cce3ea78f"
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, output_dim, input_length = input_length))\n",
    "model.add(Flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lv5pqzNZtnw"
   },
   "source": [
    "### Le réseau de neurones\n",
    "On construit maintenant notre réseau de neurones en ajoutant des couches de neurones (``Dense``), puis on le compile avec la fonction ``compile``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "FPvY6GvuZvGE",
    "outputId": "0ee53fc3-9269-4b04-fae9-50a9eeb29b18"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation= 'relu' ))\n",
    "model.add(Dense(1, activation= 'sigmoid' ))\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AMkopwBTsnh"
   },
   "source": [
    "### Visualiser le modèle \n",
    "On visualise le schéma de notre réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "-HsL5qsqTpdG",
    "outputId": "f9d7ef76-3212-4856-9258-ef7a6896643f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 32)           32000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                32010     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 64,021\n",
      "Trainable params: 64,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce réseau de neurones très simple contient déjà plus de 64000 paramètres !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g52cxipzTypq"
   },
   "source": [
    "## Entrainer l'algorithme sur les données\n",
    "On entraîne notre modèle sur les données d'entraînement (*train*), et on utilise les données de *test* pour tester/valider sa précision sur des données exogènes (que le modèle n'a jamais vues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "ZwLdXOwiNXCJ",
    "outputId": "f1dc6ec9-92ea-4df6-ec53-a0873944dba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.5034 - accuracy: 0.7386 - val_loss: 0.3825 - val_accuracy: 0.8239\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.3402 - accuracy: 0.8513 - val_loss: 0.3853 - val_accuracy: 0.8222\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.2720 - accuracy: 0.8893 - val_loss: 0.4191 - val_accuracy: 0.8117\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1910 - accuracy: 0.9311 - val_loss: 0.4854 - val_accuracy: 0.8016\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1135 - accuracy: 0.9685 - val_loss: 0.5739 - val_accuracy: 0.7939\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0574 - accuracy: 0.9895 - val_loss: 0.6871 - val_accuracy: 0.7872\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0261 - accuracy: 0.9974 - val_loss: 0.7869 - val_accuracy: 0.7871\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 2s 6ms/step - loss: 0.0116 - accuracy: 0.9992 - val_loss: 0.8892 - val_accuracy: 0.7846\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0057 - accuracy: 0.9998 - val_loss: 0.9719 - val_accuracy: 0.7860\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 1.0398 - val_accuracy: 0.7870\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs = 10, \n",
    "                    batch_size = 64,\n",
    "                    verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPqTgPA8T6ab"
   },
   "source": [
    "## Evaluer la performance de l'algorithme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule la précision du modèle après entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-Q2qFVoWT5X0",
    "outputId": "95142fff-db59-434b-8e0e-415225ca3074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc =  78.70%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Acc =  %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà ! On voit qu'en seulement quelques lignes de code on a réussi à classifier les avis des commentaires avec une précision de presque 80% !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2496_04_02-classifier-doc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
